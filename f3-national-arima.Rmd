---
output: github_document
---


```{r}
knitr::opts_chunk$set(message=FALSE)
```

```{r setup}
library(neonstore)
library(tidyverse)
library(tsibble)
library(fable)
library(neon4cast) # remotes::install_github("eco4cast/neon4cast", dep=TRUE)

source("R/iso_week.R")
team <- "cb_f3"

```


## Access Target Data

```{r}
targets <-
  "https://data.ecoforecast.org/neon4cast-targets/beetles/beetles-targets.csv.gz" |> 
  read_csv(show_col_types = FALSE) 

#targets |> duplicates(index = datetime, key  = c(variable,site_id))

targets_ts <- targets |> 
  as_tsibble(index = datetime, key = c(variable,site_id))
```


```{r}
forecast_date <- Sys.Date() - months(3)
past <-  targets_ts |>
  filter(datetime < forecast_date)  |>
  pivot_wider(names_from="variable", values_from="observation")
```




## Process vs Noise


As the f1 & f2 example plots above show, counts of abundance and richness at individual sites and weeks are pretty noisy, with lots of gaps and zeros and no strong visual pattern.  In contrast, if we average by month over all sites nationally, we see a strong and predictable seasonal pattern:
While a hierarchical forecast could allow us to pool power across sites, a simple starting point might be to try and back out the site-level predictions based on a forecast of this strong seasonal pattern.  
An ARIMA model is a good way to capture this periodic trend.
We treat the sites as merely random draws from this national pool,
weighted by the fraction that each site has contributed to the grand total observed richness.

```{r}
national_ave <- past %>%
  index_by(Time = ~ yearmonth(.)) %>% # temporal aggregates
  summarise(
    richness = mean(richness, na.rm = TRUE),
    abundance = mean(abundance, na.rm = TRUE)
  ) %>% rename(time = Time)



national_ave %>% 
  pivot_longer(c("richness", "abundance"), 
               names_to = "variable", 
               values_to="mean_observed") %>% 
  ggplot(aes(time, mean_observed)) + geom_line() +
  facet_wrap(~variable, ncol = 1, scales = "free_y")
```

While a hierarchical forecast could allow us to pool power across sites, a simple starting point might be to try and back out the site-level predictions based on a forecast of this strong seasonal pattern.  
An ARIMA model is a good way to capture this periodic trend.
We will use log-transforms as a simple way to avoid the possibility of negative values in our predictions.


```{r}
ave_richness <- national_ave %>%
  fill_gaps() %>%
  model(arima = ARIMA(log(richness + 1e-6))) %>%
  forecast(h = "1 year")

ave_richness %>% autoplot(national_ave) + ggtitle("richness")
```


```{r}
ave_abund <- national_ave %>%
  fill_gaps() %>%
  model(arima = ARIMA(log(abundance+1e-5))) %>%
  forecast(h = "1 year")

ave_abund %>% autoplot(national_ave) + ggtitle("abundance")
```

We treat the sites as merely random draws from this national pool,
weighted by the fraction that each site has contributed to the grand total observed richness.

```{r}
site_weights <- 
  past %>% as_tibble() %>% 
  group_by(site_id) %>% 
  summarise(richness_share = sum(richness, na.rm=TRUE),
            abundance_share = sum(abundance, na.rm=TRUE)) %>% 
  mutate(richness_share = richness_share / sum(richness_share),
         abundance_share = abundance_share / sum(abundance_share))
site_weights

```

```{r}
# Note distribution is normal in log-transformed variable
national_model <- ave_richness %>% 
    dplyr::mutate(sd = sqrt( distributional::variance( richness ) ) ) %>%
    dplyr::rename(mean = .mean) %>%
    dplyr::select(time, .model, mean, sd) %>%
    tidyr::pivot_longer(c(mean, sd), names_to = "parameter", values_to = "richness") %>%
    dplyr::as_tibble()

national_model <-  ave_abund %>% 
    dplyr::mutate(sd = sqrt( distributional::variance( abundance ) ) ) %>%
    dplyr::rename(mean = .mean) %>%
    dplyr::select(time, .model, mean, sd) %>%
    tidyr::pivot_longer(c(mean, sd),
                        names_to = "parameter",
                        values_to = "abundance") %>%
    dplyr::as_tibble() %>% 
  inner_join(national_model)
```



Combining the forecast for the nation-wide pooled mean richness with the site-by-site richness share, we could then generate site-level forecasts for each month.


```{r}
forecast <- national_model %>% group_by(time, parameter) %>% 
  dplyr::group_map(~ mutate(site_weights, 
                            richness = richness_share * .x$richness, 
                            abundance = abundance_share * .x$abundance,
                            parameter = .y$parameter, 
                            datetime = .y$time)) %>%
  bind_rows() %>%
  select(datetime, site_id, parameter, abundance, richness) %>% 
  mutate(datetime = as.Date(datetime), family="normal") |>
  pivot_longer(c(abundance, richness),
               names_to="variable",
               values_to="prediction") 
```

Convert time measures to iso_week

```{r}
forecast <- forecast %>%
  mutate(datetime = iso_week(datetime),
         reference_datetime = forecast_date,
         parameter = forcats::fct_recode(parameter, mu="mean", sigma = "sd"))
```



```{r}
forecast_file <- glue::glue("{theme}-{date}-{team}.csv.gz",
                            theme = "beetles", 
                            date=forecast_date,
                            team = team)
write_csv(forecast, forecast_file)
```


```{r}
neon4cast::forecast_output_validator(forecast_file)
```




```{r}
neon4cast::submit(forecast_file)
```
```{r}
unlink(forecast_file)
```

